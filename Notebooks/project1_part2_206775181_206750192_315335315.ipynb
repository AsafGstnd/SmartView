{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1f4130b-5b1f-4418-b908-cd49fd749d2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Extract fields & Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c89abd-985d-4b8e-b12b-b0ca57a4cd2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "spark = SparkSession.builder.appName(\"project_1_part_2\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1e03dc1-99b6-4bed-a246-2044d1603862",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read a CSV into a dataframe\n",
    "\n",
    "def load_csv_file(filename, schema):\n",
    "  # Reads the relevant file from distributed file system using the given schema\n",
    "\n",
    "  allowed_files = {'Daily program data': ('Daily program data', \"|\"),\n",
    "                   'demographic': ('demographic', \"|\")}\n",
    "\n",
    "  if filename not in allowed_files.keys():\n",
    "    print(f'You were trying to access unknown file \\\"{filename}\\\". Only valid options are {allowed_files.keys()}')\n",
    "    return None\n",
    "\n",
    "  filepath = allowed_files[filename][0]\n",
    "  dataPath = f\"dbfs:/mnt/coursedata2024/fwm-stb-data/{filepath}\"\n",
    "  delimiter = allowed_files[filename][1]\n",
    "\n",
    "  df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"false\")\\\n",
    "    .option(\"delimiter\",delimiter)\\\n",
    "    .schema(schema)\\\n",
    "    .load(dataPath)\n",
    "  return df\n",
    "\n",
    "# This dict holds the correct schemata for easily loading the CSVs\n",
    "schemas_dict = {'Daily program data':\n",
    "                  StructType([\n",
    "                    StructField('prog_code', StringType()),\n",
    "                    StructField('title', StringType()),\n",
    "                    StructField('genre', StringType()),\n",
    "                    StructField('air_date', StringType()),\n",
    "                    StructField('air_time', StringType()),\n",
    "                    StructField('Duration', FloatType())\n",
    "                  ]),\n",
    "                'viewing':\n",
    "                  StructType([\n",
    "                    StructField('device_id', StringType()),\n",
    "                    StructField('event_date', StringType()),\n",
    "                    StructField('event_time', IntegerType()),\n",
    "                    StructField('mso_code', StringType()),\n",
    "                    StructField('prog_code', StringType()),\n",
    "                    StructField('station_num', StringType())\n",
    "                  ]),\n",
    "                'viewing_full':\n",
    "                  StructType([\n",
    "                    StructField('mso_code', StringType()),\n",
    "                    StructField('device_id', StringType()),\n",
    "                    StructField('event_date', IntegerType()),\n",
    "                    StructField('event_time', IntegerType()),\n",
    "                    StructField('station_num', StringType()),\n",
    "                    StructField('prog_code', StringType())\n",
    "                  ]),\n",
    "                'demographic':\n",
    "                  StructType([StructField('household_id',StringType()),\n",
    "                    StructField('household_size',IntegerType()),\n",
    "                    StructField('num_adults',IntegerType()),\n",
    "                    StructField('num_generations',IntegerType()),\n",
    "                    StructField('adult_range',StringType()),\n",
    "                    StructField('marital_status',StringType()),\n",
    "                    StructField('race_code',StringType()),\n",
    "                    StructField('presence_children',StringType()),\n",
    "                    StructField('num_children',IntegerType()),\n",
    "                    StructField('age_children',StringType()), #format like range - 'bitwise'\n",
    "                    StructField('age_range_children',StringType()),\n",
    "                    StructField('dwelling_type',StringType()),\n",
    "                    StructField('home_owner_status',StringType()),\n",
    "                    StructField('length_residence',IntegerType()),\n",
    "                    StructField('home_market_value',StringType()),\n",
    "                    StructField('num_vehicles',IntegerType()),\n",
    "                    StructField('vehicle_make',StringType()),\n",
    "                    StructField('vehicle_model',StringType()),\n",
    "                    StructField('vehicle_year',IntegerType()),\n",
    "                    StructField('net_worth',IntegerType()),\n",
    "                    StructField('income',StringType()),\n",
    "                    StructField('gender_individual',StringType()),\n",
    "                    StructField('age_individual',IntegerType()),\n",
    "                    StructField('education_highest',StringType()),\n",
    "                    StructField('occupation_highest',StringType()),\n",
    "                    StructField('education_1',StringType()),\n",
    "                    StructField('occupation_1',StringType()),\n",
    "                    StructField('age_2',IntegerType()),\n",
    "                    StructField('education_2',StringType()),\n",
    "                    StructField('occupation_2',StringType()),\n",
    "                    StructField('age_3',IntegerType()),\n",
    "                    StructField('education_3',StringType()),\n",
    "                    StructField('occupation_3',StringType()),\n",
    "                    StructField('age_4',IntegerType()),\n",
    "                    StructField('education_4',StringType()),\n",
    "                    StructField('occupation_4',StringType()),\n",
    "                    StructField('age_5',IntegerType()),\n",
    "                    StructField('education_5',StringType()),\n",
    "                    StructField('occupation_5',StringType()),\n",
    "                    StructField('polit_party_regist',StringType()),\n",
    "                    StructField('polit_party_input',StringType()),\n",
    "                    StructField('household_clusters',StringType()),\n",
    "                    StructField('insurance_groups',StringType()),\n",
    "                    StructField('financial_groups',StringType()),\n",
    "                    StructField('green_living',StringType())\n",
    "                  ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2f94cc9-0731-4cba-afca-8411fa863995",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f6bdfcf-cbc4-4d01-8fd1-02dd452bbb5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.55 ms, sys: 0 ns, total: 4.55 ms\nWall time: 301 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# demographic data filename is 'demographic'\n",
    "demo_df = load_csv_file('demographic', schemas_dict['demographic'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3edbf3e5-9cc8-4934-b796-78d51b37f498",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col, count, expr, when\n",
    "\n",
    "#remove unwanted columns from demographic data\n",
    "demo_df_col = ['household_id', 'household_size', 'num_adults', 'net_worth', 'income', 'green_living']\n",
    "demo_df = demo_df.select(*demo_df_col)\n",
    "\n",
    "#remove duplicates\n",
    "demo_df = demo_df.dropDuplicates([\"household_id\"])\n",
    "\n",
    "#add column representing the income column cast to int\n",
    "demo_df = demo_df.withColumn(\n",
    "    \"income_int\",\n",
    "    expr(\"\"\"\n",
    "        CASE income\n",
    "            WHEN 'A' THEN 10\n",
    "            WHEN 'B' THEN 11\n",
    "            WHEN 'C' THEN 12\n",
    "            WHEN 'D' THEN 13\n",
    "            ELSE CAST(income AS INT)\n",
    "        END\n",
    "    \"\"\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d973b520-b730-4355-8022-6cf61f27883b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Daily program data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201dcd80-1845-431d-894a-701cf3731a13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.16 ms, sys: 269 Âµs, total: 5.43 ms\nWall time: 518 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# daily_program data filename is 'Daily program data'\n",
    "daily_prog_df = load_csv_file('Daily program data', schemas_dict['Daily program data'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "582e27b5-eb22-4042-bc87-c221a19e586a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensures that each prog_code has a consistent title value based on the most frequent title.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc, col\n",
    "\n",
    "# Group by prog_code and title to get the count of each title for each prog_code\n",
    "title_counts_df = daily_prog_df.groupBy(\"prog_code\", \"title\").count()\n",
    "\n",
    "# Define a window partitioned by prog_code and ordered by count descending\n",
    "window_spec = Window.partitionBy(\"prog_code\").orderBy(desc(\"count\"))\n",
    "\n",
    "# Add a row number within each partition to identify the most frequent title\n",
    "ranked_title_df = title_counts_df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to keep only the most frequent title for each prog_code\n",
    "most_frequent_title_df = ranked_title_df.filter(col(\"rank\") == 1).select(\"prog_code\", \"title\")\n",
    "\n",
    "# Join the result back to the original DataFrame to tie-break the rows\n",
    "daily_prog_df = daily_prog_df.drop(\"title\").join(most_frequent_title_df, on=\"prog_code\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10493676-1363-4c41-b8da-39591277708c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensures that each prog_code has a consistent genre value based on the most frequent genre.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc, col\n",
    "\n",
    "# Group by prog_code and genre to get the count of each genre for each prog_code\n",
    "genre_counts_df = daily_prog_df.groupBy(\"prog_code\", \"genre\").count()\n",
    "\n",
    "# Define a window partitioned by prog_code and ordered by count descending\n",
    "window_spec = Window.partitionBy(\"prog_code\").orderBy(desc(\"count\"))\n",
    "\n",
    "# Add a row number within each partition to identify the most frequent genre\n",
    "ranked_genres_df = genre_counts_df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to keep only the most frequent genre for each prog_code\n",
    "most_frequent_genre_df = ranked_genres_df.filter(col(\"rank\") == 1).select(\"prog_code\", \"genre\")\n",
    "\n",
    "# Join the result back to the original DataFrame to tie-break the rows\n",
    "daily_prog_df = daily_prog_df.drop(\"genre\").join(most_frequent_genre_df, on=\"prog_code\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e820ad4c-c3c1-4f2e-9877-41b00b1f0e1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import to_date, col, date_format, count, expr\n",
    "\n",
    "\n",
    "#fix date and add day of the week column\n",
    "daily_prog_df = daily_prog_df.withColumn(\"air_date\", to_date(col(\"air_date\"), \"yyyyMMdd\")) \\\n",
    "    .withColumn(\"air_time\", col(\"air_time\").cast(\"int\"))  \\\n",
    "    .withColumn(\"day\", date_format('air_date', 'EEE'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21a21cd9-93e7-4b33-8da0-dd56d756307d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#drop unnecessary column\n",
    "daily_prog_df = daily_prog_df.drop(\"duration\")\n",
    "daily_prog_df = daily_prog_df.dropna().dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06969e7e-0f01-4f24-a6e8-194efe488d7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Sample of 10 Million viewing entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed25f74f-9cc6-4c56-9592-841a23334541",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample of 10 Million viewing entries\n",
    "\n",
    "dataPath = f\"dbfs:/viewing_10M\"\n",
    "viewing10m_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"delimiter\",\",\")\\\n",
    "    .schema(schemas_dict['viewing_full'])\\\n",
    "    .load(dataPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "329df260-d64b-490c-a187-c100680ce854",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#fix date type\n",
    "viewing10m_df = viewing10m_df.withColumn(\"event_date\", to_date(col(\"event_date\"), \"yyyyMMdd\")) \n",
    "\n",
    "#drop columns that are not needed for the analysis\n",
    "cols_to_drop_viewing10m = ['mso_code', 'station_num']\n",
    "viewing10m_df = viewing10m_df.drop(*cols_to_drop_viewing10m)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f93166-3b27-4e10-9c04-a37d04cfbf6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#remove records that appear in viewing but not daily_prog\n",
    "all_program_aired = daily_prog_df.select(\"prog_code\").distinct()\n",
    "viewing10m_df = viewing10m_df.join(all_program_aired, \"prog_code\", \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f1b9a1-4403-4f64-aee6-5d77057df7f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1df74e61-2329-4915-a65d-03c9366aeab4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a35f3b1c-279f-43f6-8bc7-985fa4381728",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.58 ms, sys: 0 ns, total: 3.58 ms\nWall time: 260 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# reference data is stored in parquet for your convinence.\n",
    "\n",
    "ref_df = spark.read.parquet('dbfs:/refxml_new_parquet')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "072207b7-1bd4-4934-bfb7-36d01e1adcb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col, count, dense_rank\n",
    "\n",
    "\n",
    "#removing records with unknown dma\n",
    "ref_df = ref_df.filter(col(\"dma\") != \"Unknown\")\n",
    "\n",
    "\n",
    "# drop columns that are not needed for the analysis\n",
    "cols_to_drop_ref = ['household_type', 'system_type']\n",
    "ref_df = ref_df.drop(*cols_to_drop_ref)\n",
    "\n",
    "\n",
    "#remove duplicate device_id\n",
    "ref_df = ref_df.dropDuplicates(['device_id'])\n",
    "\n",
    "#drop records with multiple dma\n",
    "windowHousehold = Window.partitionBy(\"household_id\").orderBy(\"dma\")\n",
    "ref_df = ref_df.withColumn(\"dma_count\", dense_rank().over(windowHousehold))\n",
    "ref_df = ref_df.filter(f.col(\"dma_count\") == 1).drop(\"dma_count\")\n",
    "\n",
    "#add column for number of household's devices\n",
    "window_by_household_id = Window.partitionBy(\"household_id\")\n",
    "ref_df = ref_df.withColumn(\"num_of_devices\", f.size(f.collect_set(\"device_id\").over(window_by_household_id)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d07405f-1622-4adc-8d08-8a77067a6ef1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#drop records with multiple zipcode\n",
    "\n",
    "windowHousehold = Window.partitionBy(\"household_id\")\n",
    "\n",
    "# Count distinct zipcodes per household\n",
    "ref_df = ref_df.withColumn(\"zipcode_count\", f.approx_count_distinct(\"zipcode\").over(windowHousehold))\n",
    "\n",
    "# Keep only records from households with one unique zipcode\n",
    "ref_df = ref_df.filter(f.col(\"zipcode_count\") == 1).drop(\"zipcode_count\")\n",
    "\n",
    "#not neccessary as we are not using zipcode for analysis\n",
    "ref_df = ref_df.drop(\"zipcode\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a5f26b-53ac-40ee-889f-9e932b4774c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#remove records with houshold_id not in demo_df\n",
    "\n",
    "all_households = demo_df.select(\"household_id\").distinct()\n",
    "ref_df = ref_df.join(all_households, \"household_id\", \"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bf85dda-374c-44a7-afd9-be7090d9b221",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#remove records with houshold_id not in demo_df\n",
    "all_households = demo_df.select(\"household_id\").distinct()\n",
    "ref_df = ref_df.join(all_households, \"household_id\", \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c63b9a2b-89a5-474c-b6ba-8c8c24a3b506",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2837e6bc-a5f7-4d09-be52-d66df2e12d46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6560a40d-2407-488b-b744-404b210ccb46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Top 10 largest DMAs by amount of devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a778b2a-7c9a-47d9-934e-24a93f1ac97f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dma</th><th>count</th></tr></thead><tbody><tr><td>Charleston-Huntington</td><td>44803</td></tr><tr><td>Wilkes Barre-Scranton-Hztn</td><td>42857</td></tr><tr><td>Seattle-Tacoma</td><td>29762</td></tr><tr><td>Little Rock-Pine Bluff</td><td>27104</td></tr><tr><td>Toledo</td><td>26608</td></tr><tr><td>Amarillo</td><td>25876</td></tr><tr><td>Bend, OR</td><td>25273</td></tr><tr><td>Greenville-N.Bern-Washngtn</td><td>24555</td></tr><tr><td>Washington, DC (Hagrstwn)</td><td>23486</td></tr><tr><td>Houston</td><td>20821</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Charleston-Huntington",
         44803
        ],
        [
         "Wilkes Barre-Scranton-Hztn",
         42857
        ],
        [
         "Seattle-Tacoma",
         29762
        ],
        [
         "Little Rock-Pine Bluff",
         27104
        ],
        [
         "Toledo",
         26608
        ],
        [
         "Amarillo",
         25876
        ],
        [
         "Bend, OR",
         25273
        ],
        [
         "Greenville-N.Bern-Washngtn",
         24555
        ],
        [
         "Washington, DC (Hagrstwn)",
         23486
        ],
        [
         "Houston",
         20821
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dma",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, explode, split, count, desc\n",
    "\n",
    "# Identify the top 10 largest DMAs by amount of devices\n",
    "top_dmas = ref_df.groupBy(\"dma\").count().orderBy(desc(\"count\")).limit(10)\n",
    "display(top_dmas)\n",
    "\n",
    "device_dma = ref_df.join(top_dmas, \"dma\", \"inner\").select(\"device_id\", \"dma\")\n",
    "\n",
    "part2 = viewing10m_df.join(device_dma, \"device_id\", \"inner\") \\\n",
    "    .select(\"device_id\", \"dma\", \"prog_code\").dropDuplicates() \\\n",
    "        .join(daily_prog_df, \"prog_code\", \"inner\") \\\n",
    "            .select(\"device_id\", \"dma\", \"prog_code\", \"genre\").dropDuplicates().dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e594281-d8c2-4044-a35c-72889d36f455",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------+------------+\n|   device_id|                 dma|     prog_code|       genre|\n+------------+--------------------+--------------+------------+\n|00000361d4c9|              Toledo|EP000000211624|Newsmagazine|\n|001ac32eabf5|Wilkes Barre-Scra...|EP000000211624|Newsmagazine|\n|001bd73e0ecd|Wilkes Barre-Scra...|EP000000211624|Newsmagazine|\n|001bd7473ce8|Wilkes Barre-Scra...|EP000000211624|Newsmagazine|\n|001bd7681598|Wilkes Barre-Scra...|EP000000211624|Newsmagazine|\n+------------+--------------------+--------------+------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, explode, split\n",
    "\n",
    "part2 = part2.withColumn(\"genre\", explode(split(col(\"genre\"), \",\"))).dropDuplicates()\n",
    "part2.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68f4d402-7a78-4eb4-8c17-8dbe072f146a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|                 dma|count|\n+--------------------+-----+\n|Charleston-Huntin...|44803|\n|Wilkes Barre-Scra...|42857|\n|      Seattle-Tacoma|29762|\n|Little Rock-Pine ...|27104|\n|              Toledo|26608|\n|            Amarillo|25876|\n|            Bend, OR|25273|\n|Greenville-N.Bern...|24555|\n|Washington, DC (H...|23486|\n|             Houston|20821|\n+--------------------+-----+\n\n+--------------------+-----+--------------------+\n|                 dma|count|            dma_name|\n+--------------------+-----+--------------------+\n|Charleston-Huntin...|44803|CharlestonHuntington|\n|Wilkes Barre-Scra...|42857|Wilkes BarreScran...|\n|      Seattle-Tacoma|29762|       SeattleTacoma|\n|Little Rock-Pine ...|27104|Little RockPine B...|\n|              Toledo|26608|              Toledo|\n|            Amarillo|25876|            Amarillo|\n|            Bend, OR|25273|             Bend OR|\n|Greenville-N.Bern...|24555|GreenvilleNBernWa...|\n|Washington, DC (H...|23486|Washington DC Hag...|\n|             Houston|20821|             Houston|\n+--------------------+-----+--------------------+\n\nnow working on the 1th dma:  project1_part2_CharlestonHuntington_206775181_206750192_315335315\n+---------------------+-----------+\n|dma                  |genre      |\n+---------------------+-----------+\n|Charleston-Huntington|Reality    |\n|Charleston-Huntington|News       |\n|Charleston-Huntington|Sitcom     |\n|Charleston-Huntington|Talk       |\n|Charleston-Huntington|Drama      |\n|Charleston-Huntington|Documentary|\n|Charleston-Huntington|Adventure  |\n|Charleston-Huntington|Children   |\n|Charleston-Huntington|Animated   |\n|Charleston-Huntington|Comedy     |\n+---------------------+-----------+\nonly showing top 10 rows\n\nnow working on the 2th dma:  project1_part2_Wilkes_BarreScrantonHztn_206775181_206750192_315335315\nnow working on the 3th dma:  project1_part2_SeattleTacoma_206775181_206750192_315335315\nnow working on the 4th dma:  project1_part2_Little_RockPine_Bluff_206775181_206750192_315335315\nnow working on the 5th dma:  project1_part2_Toledo_206775181_206750192_315335315\n+------+----------------+\n|dma   |genre           |\n+------+----------------+\n|Toledo|News            |\n|Toledo|Reality         |\n|Toledo|Talk            |\n|Toledo|Sitcom          |\n|Toledo|Sports event    |\n|Toledo|Comedy          |\n|Toledo|Documentary     |\n|Toledo|Drama           |\n|Toledo|Adventure       |\n|Toledo|Sports non-event|\n+------+----------------+\nonly showing top 10 rows\n\nnow working on the 6th dma:  project1_part2_Amarillo_206775181_206750192_315335315\nnow working on the 7th dma:  project1_part2_Bend_OR_206775181_206750192_315335315\nnow working on the 8th dma:  project1_part2_GreenvilleNBernWashngtn_206775181_206750192_315335315\nnow working on the 9th dma:  project1_part2_Washington_DC_Hagrstwn_206775181_206750192_315335315\n+-------------------------+-----------+\n|dma                      |genre      |\n+-------------------------+-----------+\n|Washington, DC (Hagrstwn)|Reality    |\n|Washington, DC (Hagrstwn)|News       |\n|Washington, DC (Hagrstwn)|Sitcom     |\n|Washington, DC (Hagrstwn)|Comedy     |\n|Washington, DC (Hagrstwn)|Children   |\n|Washington, DC (Hagrstwn)|Talk       |\n|Washington, DC (Hagrstwn)|Animated   |\n|Washington, DC (Hagrstwn)|Drama      |\n|Washington, DC (Hagrstwn)|Adventure  |\n|Washington, DC (Hagrstwn)|Documentary|\n+-------------------------+-----------+\nonly showing top 10 rows\n\nnow working on the 10th dma:  project1_part2_Houston_206775181_206750192_315335315\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col, when, regexp_replace, replace, count, desc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_genre = Window.partitionBy(\"genre\")\n",
    "\n",
    "show_df_index = [1, 5, 9]\n",
    "top_dmas.show()\n",
    "top_dmas = top_dmas.withColumn(\"dma_name\", regexp_replace(col(\"dma\"), r'[^a-zA-Z0-9\\s]', ''))\n",
    "top_dmas.show()\n",
    "\n",
    "def save_dataframe_as_csv(df, filename):\n",
    "    \"\"\"\n",
    "    Save the given DataFrame as a CSV file with the specified filename.\n",
    "\n",
    "    :param df: DataFrame to save\n",
    "    :param filename: Name of the CSV file\n",
    "    \"\"\"\n",
    "    df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(filename)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    current_dma_name = 'project1_part2_' + top_dmas.collect()[i][\"dma_name\"].replace(\" \", \"_\") + \"_206775181_206750192_315335315\"\n",
    "    print(f\"now working on the {i+1}th dma:  {current_dma_name}\")\n",
    "    current_df = part2.filter(col(\"dma\") == top_dmas.collect()[i][\"dma\"]) \\\n",
    "        .withColumn(\"genre_count\", count(\"*\").over(window_genre)) \\\n",
    "        .select(\"dma\", \"genre\", \"genre_count\") \\\n",
    "        .distinct().orderBy(desc(\"genre_count\")).drop(\"genre_count\")\n",
    "    save_dataframe_as_csv(current_df, current_dma_name + \".csv\")\n",
    "    if i+1 in show_df_index:\n",
    "        current_df.show(10, truncate=False)\n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15c3d766-1d42-4542-96ab-d5ea4b87637c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a02b807-36d5-46f8-bf9a-29eb06be89db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Top 10 DMAs according to Wealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbad452-ede5-4157-928d-2a2c1ca7273c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dma</th><th>wealth_score</th></tr></thead><tbody><tr><td>San Antonio</td><td>1.623931623931624</td></tr><tr><td>Baltimore</td><td>1.4813042534625838</td></tr><tr><td>San Francisco-Oak-San Jose</td><td>1.4238841405508071</td></tr><tr><td>Sacramnto-Stkton-Modesto</td><td>1.4163826201962053</td></tr><tr><td>Bend, OR</td><td>1.3998498301731357</td></tr><tr><td>Austin</td><td>1.389955092480879</td></tr><tr><td>Seattle-Tacoma</td><td>1.3747116636464911</td></tr><tr><td>Houston</td><td>1.3579162058465934</td></tr><tr><td>Detroit</td><td>1.3305320548857542</td></tr><tr><td>Harrisburg-Lncstr-Leb-York</td><td>1.2970870798360419</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "San Antonio",
         1.623931623931624
        ],
        [
         "Baltimore",
         1.4813042534625838
        ],
        [
         "San Francisco-Oak-San Jose",
         1.4238841405508071
        ],
        [
         "Sacramnto-Stkton-Modesto",
         1.4163826201962053
        ],
        [
         "Bend, OR",
         1.3998498301731357
        ],
        [
         "Austin",
         1.389955092480879
        ],
        [
         "Seattle-Tacoma",
         1.3747116636464911
        ],
        [
         "Houston",
         1.3579162058465934
        ],
        [
         "Detroit",
         1.3305320548857542
        ],
        [
         "Harrisburg-Lncstr-Leb-York",
         1.2970870798360419
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dma",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "wealth_score",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col, when, count, desc, avg, max\n",
    "\n",
    "# Assuming demo_df and ref_df are defined elsewhere and available here\n",
    "\n",
    "demo_wealth_df = demo_df.select(\"household_id\", \"income_int\", \"net_worth\") \\\n",
    "  .join(ref_df, \"household_id\", \"inner\") \\\n",
    "  .select(\"household_id\", \"income_int\", \"net_worth\", \"dma\") \\\n",
    "  .dropDuplicates()\n",
    "\n",
    "window_dma = Window.partitionBy(\"dma\")\n",
    "\n",
    "# Calculate max net_worth and max income_int over the window_dma\n",
    "max_net_worth = demo_df.select(\"net_worth\").agg(max(\"net_worth\")).collect()[0][0]\n",
    "max_income_int = demo_df.select(\"income_int\").agg(max(\"income_int\")).collect()[0][0]\n",
    "\n",
    "# Calculate wealth_score using the corrected max functions\n",
    "top_wealth_dma = demo_wealth_df.withColumn(\n",
    "    \"wealth_score\",\n",
    "    (avg(col(\"net_worth\")).over(window_dma) / max_net_worth) + \n",
    "    (avg(col(\"income_int\")).over(window_dma) / max_income_int)\n",
    ").drop(\"income_int\", \"net_worth\", \"household_id\").dropDuplicates().orderBy(desc(\"wealth_score\")).limit(10)\n",
    "\n",
    "display(top_wealth_dma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b9b2f1-40cb-4909-837d-ffae8454d578",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_wealth_dma_genres = top_wealth_dma.join(ref_df, \"dma\", \"inner\").select(\"dma\", \"device_id\",  \"wealth_score\") \\\n",
    "    .join(viewing10m_df, \"device_id\", \"inner\")\\\n",
    "    .select(\"device_id\", \"dma\", \"prog_code\", \"wealth_score\").dropDuplicates() \\\n",
    "        .join(daily_prog_df, \"prog_code\", \"inner\") \\\n",
    "            .select(\"device_id\", \"dma\", \"prog_code\", \"genre\", \"wealth_score\") \\\n",
    "                .dropDuplicates() \\\n",
    "\n",
    "top_wealth_dma_genres = top_wealth_dma_genres.withColumn(\"genre\", explode(split(col(\"genre\"), \",\"))).dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adb696ae-1642-4075-9de6-1792d4476215",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now working on the 1th dma - save as:  project1_part22_San_Antonio_206775181_206750192_315335315.csv\n+-----------+-----------------+----------------------+\n|dma        |wealth_score     |ordered list of genres|\n+-----------+-----------------+----------------------+\n|San Antonio|1.623931623931624|News                  |\n|San Antonio|1.623931623931624|Sitcom                |\n|San Antonio|1.623931623931624|Talk                  |\n|San Antonio|1.623931623931624|Weather               |\n|San Antonio|1.623931623931624|Auto                  |\n|San Antonio|1.623931623931624|Comedy                |\n|San Antonio|1.623931623931624|Cooking               |\n|San Antonio|1.623931623931624|Drama                 |\n|San Antonio|1.623931623931624|Newsmagazine          |\n|San Antonio|1.623931623931624|Reality               |\n|San Antonio|1.623931623931624|Western               |\n+-----------+-----------------+----------------------+\n\nnow working on the 2th dma - save as:  project1_part22_Baltimore_206775181_206750192_315335315.csv\nnow working on the 3th dma - save as:  project1_part22_San_FranciscoOakSan_Jose_206775181_206750192_315335315.csv\nnow working on the 4th dma - save as:  project1_part22_SacramntoStktonModesto_206775181_206750192_315335315.csv\nnow working on the 5th dma - save as:  project1_part22_Bend_OR_206775181_206750192_315335315.csv\n+--------+-----------------+----------------------+\n|dma     |wealth_score     |ordered list of genres|\n+--------+-----------------+----------------------+\n|Bend, OR|1.399933157986309|History               |\n|Bend, OR|1.399933157986309|Science               |\n|Bend, OR|1.399933157986309|How-to                |\n|Bend, OR|1.399933157986309|Animals               |\n|Bend, OR|1.399933157986309|Horror                |\n|Bend, OR|1.399933157986309|Nature                |\n|Bend, OR|1.399933157986309|Medical               |\n|Bend, OR|1.399933157986309|Playoff sports        |\n|Bend, OR|1.399933157986309|Golf                  |\n|Bend, OR|1.399933157986309|Romance               |\n|Bend, OR|1.399933157986309|Paranormal            |\n+--------+-----------------+----------------------+\n\nnow working on the 6th dma - save as:  project1_part22_Austin_206775181_206750192_315335315.csv\nnow working on the 7th dma - save as:  project1_part22_SeattleTacoma_206775181_206750192_315335315.csv\nnow working on the 8th dma - save as:  project1_part22_Houston_206775181_206750192_315335315.csv\nnow working on the 9th dma - save as:  project1_part22_Detroit_206775181_206750192_315335315.csv\n+-------+------------------+----------------------+\n|dma    |wealth_score      |ordered list of genres|\n+-------+------------------+----------------------+\n|Detroit|1.3308673896909191|Hockey                |\n|Detroit|1.3308673896909191|Card games            |\n|Detroit|1.3308673896909191|Poker                 |\n|Detroit|1.3308673896909191|Softball              |\n|Detroit|1.3308673896909191|Boxing                |\n|Detroit|1.3308673896909191|Action sports         |\n|Detroit|1.3308673896909191|Military              |\n|Detroit|1.3308673896909191|Bowling               |\n|Detroit|1.3308673896909191|Self improvement      |\n|Detroit|1.3308673896909191|Standup               |\n|Detroit|1.3308673896909191|Volleyball            |\n+-------+------------------+----------------------+\n\nnow working on the 10th dma - save as:  project1_part22_HarrisburgLncstrLebYork_206775181_206750192_315335315.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col, when, regexp_replace, replace, count, desc, lit, concat_ws\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "show_df_index = [1, 5, 9]\n",
    "\n",
    "top_wealth_dma = top_wealth_dma.withColumn(\"dma_name\", regexp_replace(col(\"dma\"), r'[^a-zA-Z0-9\\s]', '')) \n",
    "\n",
    "def save_dataframe_as_csv(df, filename):\n",
    "    \"\"\"\n",
    "    Save the given DataFrame as a CSV file with the specified filename.\n",
    "\n",
    "    :param df: DataFrame to save\n",
    "    :param filename: Name of the CSV file\n",
    "    \"\"\"\n",
    "    df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(filename)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    current_dma_name = 'project1_part22_' + top_wealth_dma.collect()[i][\"dma_name\"].replace(\" \", \"_\") + \"_206775181_206750192_315335315\"\n",
    "    print(f\"now working on the {i+1}th dma - save as:  {current_dma_name}.csv\")\n",
    "    current_df = top_wealth_dma_genres.filter(col(\"dma\") == top_wealth_dma.collect()[i][\"dma\"]) \\\n",
    "        .withColumn(\"genre_count\", count(\"*\").over(window_genre)) \\\n",
    "        .select(\"dma\", \"wealth_score\", \"genre\", \"genre_count\") \\\n",
    "        .distinct().orderBy(desc(\"genre_count\")).drop(\"genre_count\").limit(11) \\\n",
    "            .withColumnRenamed(\"genre\", \"ordered list of genres\")\n",
    "    # Convert the 'genre' column of current_df to a list\n",
    "    genres_list = [row['ordered list of genres'] for row in current_df.collect()]\n",
    "\n",
    "    \n",
    "    # Filter the DataFrame where the 'genre' column values are not in genres_list\n",
    "    top_wealth_dma_genres = top_wealth_dma_genres.filter(~col(\"genre\").isin(genres_list))\n",
    "    save_dataframe_as_csv(current_df, current_dma_name + \".csv\")\n",
    "    if i+1 in show_df_index:\n",
    "        current_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9381d543-4f23-4c3f-8e2b-0635b6ba9872",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "project1_part2_206775181_206750192_315335315",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
